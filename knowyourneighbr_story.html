KnowYourNeighbr is an ensemble modeling approach combining supervised and semi/unsupervised learning. In most applications, I have used information from the first, supervised, step to improve the second, semi/unsupervised step. I have also combined the results of these two steps, as in a form of model stacking. The below will describe the non-stacking approach.

I have used this approach with both matching outcomes (generation of synthetic control, either for post-hoc inference or prospective experimental group assignment) and neighbor outcomes (either/both of generation of a knn prediction, display of selected neighbors).

The utility is in creating output in the form of raw data -- rows of your original inputs -- which are optimally organized with regards to the importance of each column (or predictor variable) in predicting your outcome variable.

This can be useful when:
1) Data structures suggest applicability of tree-based learning:
- complex non-linear interactions
- theory-driven hierarchical structure

and there exists:

2) Potential benefit from match generation:
- synthetic controls
- interpretable measurements of bias between/across variables

3) Potential benefit from neighbor generation:
- end-user interpretability via neighbor display
- internal (or external) quality-control (including regulatory oversight)

[NB: to achieve this outcome, I needed to modify the R package FNN's default knn.reg function, to enable access to neighbor metadata
  attr(res, "nn.index")<- matrix(Z$nn.index, ncol=k);
  attr(res, "nn.dist")<- matrix(Z$nn.dist, ncol=k);
]

Process:

First, generate a tree-based model
- Using an approach such as classification/regression trees, random forest, or xgboost, generate a model which predicts your outcome variable based on your predictor variables.

library(xgboost)
library(data.table)

dat_trva <- data.table(input)
tr_rows <- createDataPartition(input$outcome, p = .8, 
															 list = FALSE, 
															 times = 1)
va_rows <- setdiff(seq(nrow(dat_trva)),tr_rows)

tr_dm <- data.matrix(dat_trva[tr_rows,])
tr_lab <- dat_trva[tr_rows,outcome]
tr_packaged <- xgb.DMatrix(tr_dm,
													 label=tr_lab
)

va_dm <- data.matrix(dat_trva[va_rows,])
va_lab <- dat_trva[va_rows,outcome]
va_packaged <- xgb.DMatrix(va_dm,
													 label=va_lab
)


tr_va_xgb_m <- xgb.train(
	objective = "reg:squarederror", 
	eta = .1,
	early_stopping_rounds = 100,
	nrounds =  1000,
	data = tr_packaged,
	max_depth = 10,
	print_every_n = 50,
	watchlist=list(train=tr_packaged,validate=va_packaged)
)

Second, extract feature importances
- Feature importances are encoded in the model object, likely as an object attribute or function to be called.

(model_imp <- xgb.importance(model=tr_va_xgb_m))
imp_val <<- model_imp$Gain
imp_vars <- model_imp$Feature

Third, normalize and scale non-outcome variables according to feature importances
-     dat_trva_scaled <- scale(data.matrix(dat_trva))
    dat_trva_scaled[is.na(dat_trva_scaled)] <- 0
    dat_trva_scaled <- dat_trva_scaled[seq(nrow(dat_trva)),]

    tr_x <- dat_trva_scaled[tr_rows,-excl_cols]
    va_x <- matrix(dat_trva_scaled[va_rows,-excl_cols],nrow=sum(va_rows))
    tr_va_x <- dat_trva_scaled[,-excl_cols]
    oos_x <- matrix(matrix(dat_oos_scaled,nrow=ifelse(is.null(nrow(dat_oos_scaled)),1,nrow(dat_oos_scaled)))[,-excl_cols],nrow=ifelse(is.null(nrow(dat_oos_scaled)),1,nrow(dat_oos_scaled)))

    # cbind(colnames(tr_x)[match(xgb_m_imp$Feature,colnames(tr_x))],xgb_m_imp$Feature) # match function check
    to_diag <- diag(apply(xgb_m_imp[,c(3,4)],1,max))
    col_matchup <- match(xgb_m_imp$Feature,colnames(tr_x))
    tr_x <- tr_x[,col_matchup] %*% to_diag
    va_x <- va_x[,col_matchup] %*% to_diag
    oos_x <- oos_x[,col_matchup] %*% to_diag
    tr_va_x <- tr_va_x[,col_matchup] %*% to_diag

      kr <- knn.reg(train = tr_x,
                    test = va_x,
                    y = dat_trva[tr_rows,price_col],
                    k = k)
      kr_resid_centered <- kr$pred/dat_trva[va_rows,price_col]-1

Fourth, apply your matching or knn model

