KnowYourNeighbr is an ensemble modeling approach combining supervised and semi/unsupervised learning. In most applications, I have used information from the first, supervised, step to improve the second, semi/unsupervised step. I have also combined the results of these two steps, as in a form of model stacking. The below will describe the non-stacking approach.

I have used this approach with both matching outcomes (generation of synthetic control, either for post-hoc inference or prospective experimental group assignment) and neighbor outcomes (either/both of generation of a knn prediction, display of selected neighbors).

The utility is in creating output in the form of raw data -- rows of your original inputs -- which are optimally organized with regards to the importance of each column (or predictor variable) in predicting your outcome variable.

This can be useful when:
1) Data structures suggest applicability of tree-based learning:
- complex non-linear interactions
- theory-driven hierarchical structure

and there exists:

2) Potential benefit from match generation:
- synthetic controls
- interpretable measurements of bias between/across variables

3) Potential benefit from neighbor generation:
- end-user interpretability via neighbor display
- internal (or external) quality-control (including regulatory oversight)

[NB: to achieve this outcome, I needed to modify the R package FNN's default knn.reg function, to achieve output of neighbor metadata]

Process:

First, generate a tree-based model
- Using an approach such as classification/regression trees, random forest, or xgboost, generate a model which predicts your outcome variable based on your predictor variables.

library(xgboost)
library(data.table)

dat_trva <- data.table(input)
tr_rows <- createDataPartition(input$outcome, p = .8, 
															 list = FALSE, 
															 times = 1)
va_rows <- setdiff(seq(nrow(dat_trva)),tr_rows)

tr_dm <- data.matrix(dat_trva[tr_rows,])
tr_lab <- dat_trva[tr_rows,outcome]
tr_packaged <- xgb.DMatrix(tr_dm,
													 label=tr_lab
)

va_dm <- data.matrix(dat_trva[va_rows,])
va_lab <- dat_trva[va_rows,outcome]
va_packaged <- xgb.DMatrix(va_dm,
													 label=va_lab
)


tr_va_xgb_m <- xgb.train(
	objective = "reg:squarederror", 
	eta = .1,
	early_stopping_rounds = 100,
	nrounds =  1000,
	data = tr_packaged,
	max_depth = 10,
	print_every_n = 50,
	watchlist=list(train=tr_packaged,validate=va_packaged)
)

Second, extract feature importances
- Feature importances are encoded in the model object, likely as an object attribute or function to be called.

(model_imp <- xgb.importance(model=tr_va_xgb_m))
imp_val <<- model_imp$Gain
imp_vars <- model_imp$Feature

Third, normalize and scale non-outcome variables according to feature importances
- 


Fourth, apply your matching or knn model

